# (PART) Economía de la complejidad {-}
# Fundamentos lógicos y filosóficos de la complejidad {-}

Hay al menos 45 definiciones de complejidad según Seth Lloyd, como se informa en The End of Science (Horgan, 1997, págs. 303-305). Rosser Jr. (1999) defendió la utilidad en el estudio de la economía de una definición que él llamó complejidad dinámica que fue originada por Day (1994). Se trata de que un sistema económico dinámico no logra generar convergencia a un punto, un ciclo límite o una explosión (o implosión) de forma endógena a partir de sus partes deterministas. Se ha argumentado que la no linealidad era una condición necesaria pero no suficiente para esta forma de complejidad, y que esta definición constituía una "gran carpa" suficientemente amplia para abarcar las "cuatro C" de la cibernética , la catástrofe , el caos y la "pequeña carpa".”(Ahora más conocidos como agentes heterogéneos ) complejidad .

## Formas de complejidad {-}

Hay al menos 45 definiciones de complejidad según Seth Lloyd, como se informa en The End of Science (Horgan,1997, págs. 303-305). Rosser Jr. (1999) defendió la utilidad en el estudio de la economía de una definición que llamó complejidad dinámica que fue originada por Day (1994). 1 Se trata de que un sistema económico dinámico no logra generar convergencia hasta un punto, un ciclo límite o una explosión (o implosión) de forma endógena a partir de sus partes deterministas. Se ha argumentado que la no linealidad era una condición necesaria pero no suficiente para esta forma de complejidad, 2 y que esta definición constituía una "gran tienda" suficientemente amplia para abarcar las "cuatro C" 3 de la cibernética , la catástrofe , el caos y la " pequeña carpa". carpa ”(ahora más conocidos como agentes heterogéneos ) complejidad .

Norbert Wiener (1948) fundó la cibernética, que se basaba en simulaciones por computadora y fue popular entre los planificadores centrales e informáticos soviéticos mucho después de que no fuera tan admirada en Occidente. Jay Forrester (1961), inventor del simulador de vuelo, fundó la dinámica de su sistema rival , argumentando que los sistemas dinámicos no lineales pueden producir resultados "contrarios a la intuición". Probablemente su aplicación más famosa fue The Limits to Growth (Meadows et al.1972), eventualmente criticada por su excesiva agregación. Podría decirse que ambos provienen de la teoría general de sistemas (von Bertalanffy,1950, 1974), que a su vez se desarrolló a partir de la tectología , la teoría general de la organización debida a Bogdanov (1925-29).

La teoría de la catástrofe se desarrolló a partir de la teoría de la bifurcación más amplia, que se basa en suposiciones sólidas para caracterizar patrones de cómo el cambio suave de las variables de control puede generar cambios discontinuos en las variables de estado en valores críticos de bifurcación (Thom, 1975), con Zeeman (1974) modelo de mercado de valores colapsa el primer uso del mismo en economía. Los métodos empíricos para estudiar tales modelos dependen de estadísticas multimodales (Cobb et al.1983; Guastello 2011a, b). Debido a las estrictas suposiciones en las que se basa, se desarrolló una reacción violenta contra su uso, aunque Rosser Jr. (2007) argumentó que esto se volvió exagerado. 4

Si bien la teoría del caos se remonta a Poincaré (1890), se hizo prominente después de que el climatólogo Edward Lorenz (1963) descubrió la dependencia sensible de las condiciones iniciales , también conocido como "el efecto mariposa". Las aplicaciones en economía siguieron las sugerencias hechas por May (1976). Los debates sobre la medición empírica y los problemas asociados con la predicción han reducido su aplicación en economía (Dechert,1996). 5 Es posible desarrollar modelos que exhiban fenómenos combinados catastróficos y caóticos como en la histéresis caótica , 6 primero mostrado como posible en un modelo macroeconómico por Puu (1990), con Rosser Jr. et al. (2001) estimando tales patrones de inversión en la Unión Soviética en el período posterior a la Segunda Guerra Mundial.

El tipo de complejidad dinámica de carpa pequeña o agentes heterogéneos no tiene una definición precisa. De manera influyente, Arthur et al. (1997a) argumentan que tal complejidad exhibe seis características: (1) interacción dispersa entre agentes heterogéneos que interactúan localmente en algún espacio, (2) ningún controlador global que pueda explotar las oportunidades que surgen de estas interacciones dispersas, (3) organización jerárquica transversal con muchos enredos interacciones, (4) aprendizaje y adaptación continuos por parte de los agentes, (5) novedad perpetua en el sistema a medida que las mutaciones lo llevan a desarrollar nuevos nichos ecológicos, y (6) dinámicas fuera de equilibrio con muchos equilibrios o ninguno y poca probabilidad de un estado global óptimo emergente. Muchos apuntan a Thomas Schelling (1971) estudio sobre un tablero Go 19 por 19 7 sobre el surgimiento de la segregación urbana debido a los efectos del vecino más cercano como un ejemplo temprano.

Otras formas de complejidad dinámica no lineal observadas en modelos económicos incluyen atractores extraños no caóticos (Lorenz 1983), límites de cuencas fractales (Lorenz 1983; Abraham et al.1997), atractores de llamaradas (Hartmann y Rössler1998; Rosser Jr. y col.2003a), y más.

Otros enfoques de complejidad no dinámica utilizados en economía han incluido estructuras (Pryor1995; Stodder1995), 8 jerárquicos (Simon1962), informativo (Shannon1948). Algorítmico (Chaitin1987), estocástico (Rissanen1986) y computacional (Lewis1985; Albin con Foley1998; Velupillai2000).

Aquellos que defienden el enfoque en la complejidad computacional incluyen Velupillai (2005a, B) y Markose (2005), quienes dicen que este último concepto es superior por su fundamento en ideas más definidas, como la complejidad algorítmica (Chaitin1987) y complejidad estocástica (Rissanen1989, 2005). Estos se consideran fundados más profundamente en el trabajo de entropía informacional de Shannon (1948) y Kolmogorov (1983). Mirowski2007) argumenta que los mercados mismos deben verse como algoritmos que están evolucionando a niveles más altos en un Chomsky (1959) jerarquía de los sistemas computacionales, especialmente a medida que se llevan cada vez más a través de las computadoras y se resuelven a través de sistemas programados de doble subasta y similares. McCauley (2004, 2005) e Israel (2005) argumentan que las ideas de complejidad dinámica como la emergencia son esencialmente vacías y deberían abandonarse por otras más basadas en la computación o más basadas en la física, las últimas basándose especialmente en conceptos de invariancia .

En el nivel más profundo, la complejidad computacional involucra el problema de la no computabilidad. En última instancia, esto depende de una base lógica, la de la no recursividad debido a la incompletitud en el sentido de Gödel (Church1936; Turing1937). En los programas informáticos reales, esto se manifiesta más claramente en la forma del problema de la detención (Blum et al.1998). Esto equivale a que el tiempo de parada de un programa sea infinito y se vincula estrechamente con otros conceptos de complejidad computacional, como la complejidad algorítmica de Chaitin. Tales problemas de incompletitud presentan problemas fundamentales para la teoría económica (Rosser Jr.2009a, 2012a, B; Landini y col.2020; Velupillai2009).

En contraste, la complejidad dinámica y conceptos tales como emergencia son útiles para comprender los fenómenos económicos y no son tan incoherentes e indefinidos como se ha argumentado. Un subtema de parte de esta literatura, aunque no toda, ha sido que los modelos o argumentos de base biológica son fundamentalmente incorrectos matemáticamente y deben evitarse en una economía más analítica. En cambio, tales enfoques pueden usarse junto con el enfoque de complejidad dinámica para explicar la emergencia matemáticamente y que tales enfoques pueden explicar ciertos fenómenos económicos que no se pueden explicar fácilmente de otra manera.

## Fundamentos de la economía de la complejidad computacional {-}

Velupillai2000, págs. 199-200) resume los fundamentos de lo que ha denominado economía computable 9 a continuación.

> La computabilidad y la aleatoriedad son las dos nociones epistemológicas básicas que he usado como bloques de construcción para definir la economía computable. Ambas nociones pueden ponerse en práctica para formalizar la teoría económica de manera eficaz. Sin embargo, solo se pueden hacer sobre la base de dos tesis: la tesis de Church-Turing y la tesis de Kolmogorov-Chaitin-Solomonoff.

Iglesia (1936) y Turing (1937) se dieron cuenta de forma independiente de que varias clases amplias de funciones podían describirse como "recursivas" y eran "calculables" (las computadoras programables aún no se habían inventado). Turing (1936, 1937) fue el primero en darse cuenta de que Gödel (1931) El teorema de la incompletitud proporcionó una base para la comprensión cuando los problemas no eran "calculables", llamados "efectivamente computables" ya que Tarski (1949). El análisis de Turing que introduce el concepto generalizado de la máquina de Turing , ahora visto como el modelo de un agente económico racional dentro de la economía computable (Velupillai2005b, pag. 181). Si bien el teorema de Gödel original se basó en una prueba diagonal de Cantor que surge de la autorreferencia, la manifestación clásica de no computabilidad en la programación es el problema que se detiene : que un programa simplemente se ejecutará para siempre sin llegar nunca a una solución (Blum et al.1998).

Gran parte de la economía computable reciente ha implicado demostrar que cuando se intenta poner partes importantes de la teoría económica estándar en formas que puedan ser computables, se descubre que no son computables de manera efectiva en ningún sentido general. Estos incluyen los equilibrios walrasianos (Lewis1992), Equilibrios de Nash (Prasad 1991; Tsuji y col.1998), aspectos más generales de la macroeconomía (Leijonufvud 1993), y si un sistema dinámico será caótico o no (da Costa et al. 2005). 10

De hecho, lo que se considera como complejidades dinámicas puede surgir de problemas de computabilidad que surgen al saltar de un marco de números reales clásico y continuo a un marco de solo números racionales digitalizado. Un ejemplo es la curiosa "función financiera" de Clower y Howitt (1978) en el que las variables de solución saltan hacia adelante y hacia atrás en grandes intervalos de forma discontinua a medida que las variables de entrada van de números enteros a racionales no enteros a números irracionales y viceversa. Velupillai2005b, pag. 186) señala el caso de un misil Patriot que falló su objetivo por 700 my mató a 28 soldados como "fuego amigo" en Dhahran, Arabia Saudita en 1991 debido a un ciclo sin interrupción de una computadora a través de una expansión binaria en una fracción decimal. Finalmente, el descubrimiento de la dependencia sensible caótica de las condiciones iniciales por Lorenz (1963) debido al error de redondeo de la computadora es famoso, un caso que es computable pero indecidible.

En realidad, existen varias definiciones de complejidad basadas en la computabilidad, aunque Velupillai (2000, 2005a, B) sostiene que pueden vincularse como parte de la base más amplia de la economía computable. El primero es el Shannon (1948) medida del contenido de la información, que puede interpretarse como un intento de observar la estructura en un sistema estocástico. Por tanto, se deriva de una medida de entropía en el sistema, o de su estado de desorden. Por lo tanto, si p ( x ) es la función de densidad de probabilidad de un conjunto de K estados denotados por valores de x , entonces la entropía de Shannon está dada por

$$H(X)=-\sum_{x=1}^{K} \ln (p(x))$$

De aquí es trivial obtener el contenido de información de Shannon de X  =  x como

$$\mathrm{SI}(x)=\ln (1 / p(x))$$

Se llegó a entender que esto equivale al número de bits en un algoritmo que se necesitan para calcular este código. Esto llevaría a Kolmogorov (1965) para definir lo que ahora se conoce como complejidad de Kolmogorov como el número mínimo de bits en cualquier algoritmo que no prefija ningún otro algoritmo a ( x ) que una Máquina de Turing Universal (UTM) requeriría para calcular una cadena binaria de información, x , o,

$$\mathrm{K}(x)=\min |a(x)|$$

donde │ │ denota la longitud del algoritmo en bits. 11 Chaitin (1987) descubriría y ampliaría de forma independiente este concepto de longitud mínima de descripción (MDL) y lo vincularía con los problemas de incompletitud de Gödel, su versión se conoce como complejidad algorítmica , que sería retomada más tarde por Albin (mil novecientos ochenta y dos) 12 y Lewis (1985, 1992) en contextos económicos. 13

Si bien estos conceptos vincularon de manera útil la teoría de la probabilidad y la teoría de la información con la teoría de la computabilidad, todos comparten el desafortunado aspecto de no ser computable. Esto se remediaría con la introducción de la complejidad estocástica por Rissanen (1978, 1986, 1989, 2005). La intuición detrás de la modificación de Rissanen de los conceptos anteriores es centrarse no en la medida directa de la información, sino en buscar una descripción o modelo más breve que describa las "características regulares" de la cuerda. Para Kolmogorov, un modelo de cadena es otra cadena que contiene la primera cadena. Rissanen (2005, págs. 89-90) define una función de verosimilitud para una estructura dada como una clase de funciones de densidad paramétricas que pueden verse como modelos respectivos, donde θ representa un conjunto de k parámetros yx es una cadena de datos dada indexada por n :

$$M_{k}=\left\{f\left(x^{n}, \theta\right): \theta \in \mathbf{R}^{k}\right}$$

Para una f dada , con f ( y n ) un conjunto de "cadenas normales", la función de máxima verosimilitud normalizada estará dada por

$$f^{*}\left(x^{n}, M_{k}\right)=f\left(x^{n}, \theta^{*}\left(x^{n}\right)\right) /\left[\int_{\theta(m)} \mathrm{f}\left(y^{n}, \theta\left(y^{n}\right)\right) \mathrm{d} y^{n}\right]$$

donde el denominador del lado derecho se puede definir como C n , k .

A partir de esto, la complejidad estocástica viene dada por

$$-\ln f^{*}\left(x^{n}, M_{k}\right)=-\ln f\left(x^{n}, \theta^{*}\left(x^{n}\right)\right)+\ln C_{n, k}$$

Este término puede interpretarse en el sentido de que representa "la 'longitud de código más corta' para los datos x n que se puede obtener con la clase de modelo M k ". (Rissanen2005, pag. 90). Con esto tenemos una medida computable de complejidad derivada de las ideas más antiguas de Kolmogorov, Solomonoff y Chaitin. La conclusión de la complejidad de Kolmogorov es que un sistema es complejo si no es computable. Los partidarios de estos enfoques para definir la complejidad económica (Israel2005; Markose2005; Velupillai2005a, B) señalan la precisión que dan estas medidas en contraste con muchas de las alternativas.

Sin embargo, la complejidad algorítmica de Chaitin (1966, 1987) introduce un límite a esta precisión, una aleatoriedad subyacente última. Consideró el problema de que un programa se haya iniciado sin que uno sepa qué es y, por lo tanto, se enfrenta a una probabilidad de que se detenga, lo que etiquetó como Ω. Vio esta aleatoriedad como subyacente a todos los "hechos" matemáticos. De hecho, este Ω en sí mismo, en general, no es computable (Rosser Jr.2020a).

Un ejemplo de esto involucra un teorema de Maymin (2011) que se extiende a ambos lados del límite del problema profundo sin resolver de si P (polinomio) es igual a NP (no polinomio) en los programas, 14 por lo que tiene un Ω desconocido. Este teorema muestra que bajo ciertas condiciones de información los mercados son eficientes si P = NP, lo que pocos creen. En el borde de este da Costa y Doria (2016) usa el O'Donnell (1979) algoritmo que es exponencial y, por lo tanto, no P pero que crece lentamente hasta “casi P” para establecer una función de contraejemplo para el problema P = NP. El algoritmo de O'Donnell se cumple si P <NP es probable para cualquier teoría estrictamente más fuerte que la aritmética recursiva primitiva, incluso si eso no puede probarlo. Tales problemas pueden aparecer como en el problema del viajante de comercio computacionalmente complejo. Da Costa y Doria establecen que en estas condiciones el algoritmo de O'Donnell se comporta como un sistema "casi P" que implica un resultado de "mercados casi eficientes". Este es un resultado que camina al borde de lo desconocido, si no de lo incognoscible.

Una cuestión lógica más profunda que subyace a la complejidad computacional y la economía implica debates fundamentales sobre la naturaleza de las matemáticas en sí. Las matemáticas convencionales asumen axiomas etiquetados como el sistema Zermelo-Fraenkel- [Axiom of] Choice, o ZFC. Pero algunos de estos axiomas han sido cuestionados y se han hecho esfuerzos para desarrollar sistemas matemáticos axiomáticos que no los utilicen. Los axiomas que han sido cuestionados han sido el axioma de la elección, el axioma del infinito y la ley del medio excluido. Un término general para estos esfuerzos ha sido matemática constructivista , con sistemas que enfatizan particularmente no depender de la Ley del Medio Excluido, lo que significa que no hay uso de prueba por contradicción, se ha conocido como intuicionismo , inicialmente desarrollado por Luitzen Brouwer (1908) de la fama del teorema del punto fijo. 15 En particular, las demostraciones estándar del teorema de Bolzano-Weierstrass utilizan la prueba por contradicción, con este Lema de Sperner subyacente, que a su vez subyace en las demostraciones estándar de los teoremas de punto fijo de Brouwer y Kakutani utilizados en las pruebas de existencia de equilibrio general y de Nash (Velupillai2006, 2008). dieciséis

Para los matemáticos, si no para los economistas, el más importante de estos axiomas discutibles es el axioma de elección, que permite la ordenación relativamente fácil de conjuntos infinitos. Esto sustenta las demostraciones estándar de los principales teoremas de la economía matemática, con Scarf (1973) probablemente el primero en notar estos posibles problemas. El axioma de elección es especialmente importante en topología y partes centrales del análisis real. Por un lado, Specker (1953). Pero una forma de solucionar algunos de estos problemas es utilizar un análisis no estándar que permita números reales infinitos e infinitesimales (Robinson1966), lo que permite evitar el uso del axioma de elección para demostrar algunos teoremas importantes.

La cuestión del axioma del infinito quizás esté más estrechamente ligada a las cuestiones sobre la complejidad computacional. La idea filosófica profunda detrás de estos enfoques constructivistas es que las matemáticas deberían tratar con sistemas finitos que son más realistas y más fáciles de calcular. En contra de esto, fue la introducción de Cantor de niveles de infinito en las matemáticas, una innovación que llevó a Hilbert a elogiar a Cantor por "traer a los matemáticos al paraíso". Pero los críticos de la computabilidad argumentan que la economía matemática debe ajustarse al mundo real de una manera creíble, con esfuerzos en curso para construir dicha economía basada en una base constructivista (Velupillai2005a, B, 2012; Bartholo y col.2009; Rosser Jr.2010a, 2012a).

## Epistemología y complejidad computacional {-}

En cuanto a la complejidad computacional, Velupillai (2000) proporciona definiciones y discusión general y Koppl y Rosser Jr. (2002) proporcionan una formulación más precisa del problema, basándose en los argumentos de Kleene (1967), Binmore (1987), Lipman (1991) y enlatado (1992). Velupillai define la complejidad computacional directamente como "intratabilidad" o insolubilidad. Detener problemas como el estudiado por Blum et al. (1998) proporcionan excelentes ejemplos de cómo puede surgir tal complejidad, con este problema estudiado por primera vez para sistemas recursivos por Church (1936) y Turing (1936, 1937).

En particular, Koppl y Rosser reexaminaron el famoso problema “Holmes-Moriarty” de la teoría de juegos, en el que dos jugadores que se comportan como máquinas de Turing contemplan un juego entre ellos que implica una regresión infinita del pensamiento sobre lo que está pensando el otro (Morgenstern 1935). Esencialmente, este es el problema del juego de n niveles con n sin límite superior (Bacharach y Stahl2000). Esto tiene un equilibrio de Nash, pero las máquinas de Turing “hiperracionales” no pueden llegar a saber que tienen esa solución o no debido al problema de la detención. Que las mejores funciones de respuesta no sean computables surge del problema de autorreferencia involucrado fundamentalmente similar a las que subyacen al Teorema de incompletitud de Gödel (Rosser Sr1936; Kleene1967, pag. 246). Aaronson (2013) ha mostrado vínculos entre estos problemas en la teoría de juegos y el problema N = P de complejidad computacional. Tales problemas se extienden también a la teoría del equilibrio general (Lewis1992; Richter y Wong1999; Landini y col.2020).

Binmore's (1987, págs. 209-212), la respuesta a tal indecidibilidad en los sistemas de autorreferencia invoca una forma "sofisticada" de actualización bayesiana que implica un grado de mayor ignorancia. Koppl y Rosser están de acuerdo en que los agentes pueden operar en tal ambiente aceptando límites en el conocimiento y operar en consecuencia, tal vez sobre la base de la intuición o "espíritus animales keynesianos" (Keynes1936). Los agentes hiperracionales no pueden tener un conocimiento completo, esencialmente por la misma razón por la que Gödel demostró que ningún sistema lógico puede ser completo en sí mismo.

Sin embargo, incluso para la solución propuesta por Binmore también existen límites. Así, Diaconis y Freedman (1986) han demostrado que el teorema de Bayes no se sostiene en un espacio de dimensión infinita. Puede haber una falla para converger en la solución correcta a través de la actualización bayesiana, especialmente cuando la base es discontinua. Puede haber convergencia en un ciclo en el que los agentes están saltando de una probabilidad a otra, ninguna de las cuales es correcta. En el ejemplo simple del lanzamiento de una moneda, es posible que estén saltando de un lado a otro asumiendo a priori de 1/3 y 2/3 sin poder nunca converger en la probabilidad correcta de 1/2. Nyarko (1991) ha estudiado este tipo de dinámicas cíclicas en situaciones de aprendizaje en modelos económicos generalizados.

Koppl y Rosser comparan este tema con el problema de Keynes (1936, Cap. 12) del concurso de belleza. En esto, se supone que los participantes ganan si adivinan con mayor precisión las conjeturas de los otros participantes, lo que potencialmente implica un problema de regresión infinita con los participantes tratando de adivinar cómo los otros participantes adivinarán sobre sus conjeturas, etc. Esto también puede verse como un problema de reflexividad (Rosser Jr.2020b). Una solución viene al optar por ser algo ignorante o limitadamente racional y operar en un nivel particular de análisis. Sin embargo, como no hay forma de determinar racionalmente el grado de acotación, lo que en sí mismo implica un problema de regresión infinita (Lipman1991), esta decisión también implica en última instancia un acto arbitrario, basado en espíritus animales o lo que sea, una decisión que finalmente se toma sin pleno conocimiento.

Un punto curiosamente relacionado aquí está en los resultados posteriores (Gode y Sunder 1993; Mirowski2002) sobre el comportamiento de los traders de inteligencia cero. Gode ​​y Sunder han demostrado que en muchas configuraciones de mercado artificial, los comerciantes de inteligencia cero que siguen reglas muy simples pueden converger en equilibrios de mercado que incluso pueden ser eficientes. No solo puede ser necesario limitar el conocimiento de uno para comportarse de una manera racional, sino que uno puede ser capaz de ser racional en algún sentido sin tener conocimiento alguno. Mirowski y Nik-Kah (2017) argumentan que esto completa una transformación del tratamiento del conocimiento en economía en la era posterior a la Segunda Guerra Mundial, pasando de suponer que todos los agentes tienen conocimiento completo a todos los agentes que tienen conocimiento cero.

Otro punto sobre esto es que hay grados de complejidad computacional (Velupillai 2000; Markose2005), con Kolmogorov (1965) proporcionando una definición ampliamente aceptada de que el grado de complejidad computacional viene dado por la duración mínima de un programa que se detendrá en una máquina de Turing. Hemos estado considerando los casos extremos de no detenerse, pero de hecho existe una jerarquía aceptada entre los niveles de complejidad computacional, y las dificultades de conocimiento experimentan cambios cualitativos a través de ellos. Se considera que esta jerarquía consta de cuatro niveles (Chomsky1959; Wolfram1984; Mirowski2007). En el nivel más bajo se encuentran los sistemas lineales, de fácil resolución, con un nivel tan bajo de complejidad computacional que podemos verlos como no complejos. Por encima de ese nivel se encuentran los problemas polinomiales (P) que son sustancialmente más complejos desde el punto de vista computacional, pero que en general se pueden resolver. Por encima de eso están los problemas exponenciales y otros problemas no polinomiales (NP) que son muy difíciles de resolver, aunque aún no se ha demostrado que estos dos niveles sean fundamentalmente distintos, uno de los problemas no resueltos más importantes de la informática. Por encima de este nivel se encuentra el de complejidad computacional total asociado donde la longitud mínima es infinita, donde los programas no se detienen. En este caso, los problemas de conocimiento solo pueden resolverse volviéndose efectivamente menos inteligente.

## Fundamentos de la economía de la complejidad dinámica {-}

En contraste con las medidas definidas computacionalmente descritas anteriormente, la definición de complejidad dinámica se destaca curiosamente en cuanto a su negatividad: sistemas dinámicos que no generan de manera endógena y determinista ciertos resultados de “buen comportamiento”. La acusación de que no es precisa tiene peso. Sin embargo, la virtud de la misma es precisamente su generalidad garantizada por su vaguedad. Puede aplicarse a una amplia variedad de sistemas y procesos que muchos han descrito como "complejos". Por supuesto, los computacionalistas argumentan con razón que son capaces de subsumir porciones sustanciales de dinámica no lineal con su enfoque, como por ejemplo con el resultado ya mencionado sobre la no computabilidad de la dinámica caótica (Costa et al.2005).

Sin embargo, la mayor parte de este debate y discusión reciente, especialmente por Israel (2005), McCauley (2005) y Velupillai (2005b, 2005c) se ha centrado en un resultado particular que está asociado con algunos modelos de agentes que interactúan dentro de la parte de complejidad de la tienda más pequeña (agentes que interactúan heterogéneos) del concepto más amplio de complejidad dinámica de la tienda grande. Esta propiedad o fenómeno es emergencia . Fue muy discutido por cibernéticos y teóricos de sistemas generales (von Bertalanffy1974), incluso bajo la etiqueta anagenesis (Boulding1978; Jantschmil novecientos ochenta y dos), aunque inicialmente fue formalizado por Lewes (1875) y ampliado por Morgan (1923), basándose en la idea de leyes heteropáticas debidas a Mill (1843, Libro III). Gran parte de la discusión reciente se ha centrado en Crutchfield (1994) porque lo ha asociado más claramente con procesos dentro de sistemas computarizados de agentes heterogéneos en interacción y lo ha vinculado a conceptos de computabilidad de longitud mínima relacionados con la idea de Kolmogorov, lo que facilita el manejo de los computacionalistas. En cualquier caso, la idea es la aparición dinámica de algo nuevo de forma endógena y determinista del sistema, a menudo también denominado autoorganización . 17

Además, todos los citados aquí agregarían otro elemento importante, que aparece en un nivel superior dentro de un sistema jerárquico dinámico como resultado de procesos que ocurren en niveles inferiores del sistema. Crutchfield1994) permite que lo que está involucrado son bifurcaciones que rompen la simetría, lo que lleva a McCauley (2005, pp.77-78) para ser especialmente despectivo, identificándolo con modelos biológicos (Kaufmann 1993) y declarando que "hasta ahora nadie ha presentado un ejemplo claro empíricamente relevante o incluso teóricamente claro". Los críticos se quejan del holismo implícito e Israel lo identifica con el de Wigner (1960) Alienación “mística” de la visión sólidamente fundamentada de Galileo.

Ahora bien, la queja de McCauley equivale a una aparente falta de invariancia , una falta de ergodicidad o equilibrios de estado estacionario, con simetrías claramente identificables cuya ruptura provoca estas reorganizaciones o transformaciones de nivel superior.

> Podemos entender cómo una célula muta a una nueva forma, pero no tenemos un modelo de cómo un pez se convierte en pájaro. Eso no quiere decir que no haya sucedido, solo que no tenemos un modelo que nos ayude a imaginar los detalles, los cuales deben basarse en complicadas interacciones celulares que no se comprenden. (McCauley2005, pag. 77) 18

Si bien probablemente tenga razón en que los detalles de estas interacciones no se comprenden completamente, una nota al pie en la misma página apunta en la dirección de algún entendimiento que ha aparecido, no vinculado directamente a Crutchfield o Kaufmann. McCauley señala el trabajo de Hermann Haken (1983) y sus "ejemplos de bifurcaciones para la formación de patrones a través de la ruptura de la simetría". En este punto se sugieren varios enfoques posibles.

Un enfoque es el de la sinergia debido a Haken (1983), aludido anteriormente. Esto trata más directamente con el concepto de arrastre de oscilaciones a través del principio esclavista (Haken1996), que opera según el principio de aproximación adiabática . Un sistema complejo se divide en parámetros de orden que se supone que se mueven lentamente en el tiempo y "esclavizan" variables o subsistemas que se mueven más rápido. Si bien puede ser que los parámetros de orden estén operando en un nivel jerárquico más alto, lo que sería consistente con muchas generalizaciones hechas sobre patrones relativos entre tales niveles (Allen y Hoekstra1990; Gritando1992; Radner1992), Este no es necesariamente el caso. Las variables pueden ser completamente equivalentes en una sola jerarquía plana, como con las variables de control y de estado en los modelos de teoría de catástrofes. Las perturbaciones estocásticas pueden provocar cambios estructurales cerca de los puntos de bifurcación.

Si la dinámica lenta viene dada por el vector F , la dinámica rápida generada por el vector q , siendo A , B y C matrices, y ε un vector de ruido estocástico, entonces una versión linealizada localmente viene dada por

$$\mathrm{d} \mathbf{q}=\mathbf{A q}+\mathbf{B}(\mathbf{F}) \mathbf{q} \mathbf{C}(\mathbf{F})+\varepsilon$$

La aproximación adiabática viene dada por

$$\mathrm{d} \mathbf{q}=-(\mathbf{A}+\mathbf{B}(\mathbf{F}))^{-1} \mathbf{C}(\mathbf{F})$$

La dependencia de la variable rápida de las variables lentas viene dada por A  +  B (F) . Los parámetros de orden son los de menor valor absoluto.

La bifurcación de ruptura de simetría ocurre cuando los parámetros de orden se desestabilizan al obtener autovalores con partes reales positivas, mientras que las “variables esclavas” exhiben lo contrario. El caos es un resultado posible. Sin embargo, la situación más dramática es cuando las variables esclavas se desestabilizan y se "rebelan" (Diener y Poston1984), con la posibilidad de que los roles cambien dentro del sistema y los antiguos esclavos reemplacen a los antiguos "jefes" para convertirse en los nuevos parámetros de orden. Un ejemplo en la naturaleza de tal arrastre emergente y autoorganizado podría ser la aparición periódica y coordinada del moho de limo a partir de amebas separadas, que luego se desintegra de nuevo en sus células aisladas (Garfinkel1987). Un ejemplo en las sociedades humanas puede ser el estallido de la Gran Plaga de mediados del siglo XIV en Europa, cuando la hambruna acumulada y la inmunodeficiencia explotó en un colapso poblacional masivo (Braudel1967).

Otro enfoque se encuentra en Nicolis (1986), derivado del trabajo de Nicolis y Prigogine (1977) sobre el arrastre de frecuencia. Rosser Jr. (1994) han argumentado que esto puede servir como un posible modelo para el momento anagenético, o el surgimiento de un nuevo nivel de jerarquía. Sea n niveles bien definidos de la jerarquía, con L 1 en la parte inferior y L n en la parte superior. Un nuevo nivel, L n +1 , o estructura disipativa , puede emerger en una transición de fase con un grado suficiente de arrastre de las oscilaciones en ese nivel. Sea k variables oscilantes, x j y z i ( t) ser un proceso estocástico exógeno distribuido de forma independiente e idéntica con media cero y varianza constante, entonces la dinámica viene dada por las ecuaciones diferenciales no lineales acopladas de la forma

$$\mathrm{d} x_{i} / \mathrm{d} t=f_{i}\left(x_{j}, t\right)+z_{i}(t)+\sum_{j=1}^{k} \int_{1}^{\mathrm{k}} x_{j}\left(t^{\prime}\right) \mathrm{w}_{i j}\left(t^{\prime}+\tau\right) \mathrm{d} t^{\prime}$$

con w ij representando un operador de matriz de correlación cruzada. El tercer término es la clave, ya sea "activado" o "desactivado", y el primero muestra el arrastre de frecuencia. Nicolis (1986) ve esto en términos de un modelo de neuronas, con un oscilador maestro no lineal duro que se enciende mediante una ruptura de simetría del operador de matriz de correlación cruzada cuando la distribución de probabilidad de las partes reales de sus valores propios es superior a cero. 19 Entonces emergerá un nuevo vector variable en el nivel L n +1 que es y j , que amortiguará o estimulará las oscilaciones en el nivel L n , dependiendo de si la suma sobre ellas está por debajo o por encima de cero. 20 Un ejemplo podría ser el surgimiento de un nuevo nivel de jerarquía urbana (Rosser Jr.1994).

Con respecto a la relación entre complejidad dinámica y emergencia, otra perspectiva sobre esto ha venido de la Escuela Austriaca de economía (Koppl 2006, 2009; Luis2012; Rosser Jr.2012a), con la idea de que los sistemas económicos de mercado emergen espontáneamente, una de sus ideas más profundas, que extrajeron de la Ilustración escocesa de Hume y Smith, así como de pensadores como Mill (1843) y Herbert Spencer (1867-1874) que escribió sobre la evolución y la sociología económica (Rosser Jr. 2014b). Este enlace se puede encontrar en el trabajo de Carl Menger (1871/1981), fundador de la Escuela Austriaca. Menger planteó esto de la siguiente manera en términos de lo que la investigación económica debería descubrir (Menger1883/1985, pag. 148):

> … Cómo las instituciones que sirven al bienestar común y son extremadamente importantes para su desarrollo surgen sin una voluntad común dirigida a establecerlas.

Menger (1892) luego planteó el surgimiento espontáneo de dinero mercantil en sociedades primitivas sin un papel fiduciario de los estados como un ejemplo importante de esto.

Varios seguidores de Menger no siguieron este enfoque con fuerza, muchos enfatizaron enfoques de equilibrio no muy diferentes de la visión neoclásica emergente, que era una idea que se podía encontrar en el trabajo de Menger, quien es ampliamente visto como uno de los fundadores del enfoque marginalista neoclásico. junto con Jevons y Walras. La figura crucial que revivió el interés por el surgimiento entre los austriacos y lo desarrolló mucho más fue Friedrich A. Hayek (1948, 1967). 21 Hayek se basó en los resultados de incompletitud de Gödel, consciente del papel de la autorreferencia en esto, y de cómo la superación de las paradojas de la incompletitud puede implicar el surgimiento de un nivel superior que pueda comprender el nivel inferior. Curiosamente, su conciencia de esto provino originalmente de su trabajo en psicología en su The Sensory Order de 1952 (pp, 188-189):

> Aplicar los mismos principios generales al cerebro humano como aparato de clasificación. Parecería significar que, aunque comprendamos su modus operandi en términos generales, o, en otras palabras, poseamos una explicación del principio sobre el que opera, nunca, por ningún medio del mismo cerebro, podremos entenderlo. llegar a una explicación detallada de su funcionamiento en circunstancias particulares, o ser capaz de predecir cuáles serán los resultados de sus operaciones. Para lograr esto, sería necesario un cerebro de una complejidad de orden superior, aunque aún podría estar construido sobre los mismos principios. Tal cerebro podría explicar lo que sucede en nuestro cerebro, pero a su vez sería incapaz de explicar sus propias operaciones, y así sucesivamente.

Koppl (2006, 2009) argumenta que este argumento se aplica también a la larga oposición de Hayek a la planificación central, con un planificador central que enfrenta este problema cuando intenta comprender el efecto en la economía que está tratando de planificar de sus propios esfuerzos de planificación. 22 Esta visión de la importancia de la complejidad y el surgimiento llegaría a tener una gran influencia en la economía austriaca desde que Hayek presentó sus argumentos y sigue siéndolo (O'Driscoll y Rizzo1985; Lachmann1986; Lavoie1989; Horwitz1992; Wagner2010).

## Conocimiento y complejidad dinámica {-}

En sistemas dinámicamente complejos, el problema del conocimiento se convierte en el problema epistemológico general. Considere el problema específico de poder conocer las consecuencias de una acción tomada en tal sistema. Sea G ( x t ) el sistema dinámico en un espacio de n dimensiones. Deje que un agente posee un conjunto de acciones A . Dejemos que una acción dada por el agente en un momento particular sea dada por un él . Por el momento, no especifiquemos ninguna acción de ningún otro agente, cada uno de los cuales también posee su propio conjunto de acciones. Podemos identificar una relación en la que x t  =  f ( a it). El problema de conocimiento para el agente en cuestión se convierte así, "¿Puede el agente conocer el sistema reducido G ( f ( a it ) cuando este sistema posee una dinámica compleja debido a la no linealidad"?

En primer lugar, es posible que el agente pueda comprender el sistema y saber que lo comprende, al menos hasta cierto punto. Una razón por la que esto puede suceder es que muchos sistemas dinámicos no lineales complejos no siempre se comportan de manera errática o discontinua. Muchos sistemas fundamentalmente caóticos exhiben transitoriedad (Lorenz1992). Un sistema puede entrar y salir de comportarse de manera caótica, pasando largos períodos durante los cuales el sistema se comportará efectivamente de una manera no compleja, ya sea siguiendo un equilibrio simple o siguiendo un ciclo límite fácilmente predecible. Si bien el sistema permanece en este patrón, las acciones del agente pueden tener resultados predecibles fácilmente, y el agente puede incluso tener confianza en su capacidad para manipular el sistema de manera sistemática. Sin embargo, esto esencialmente evita la pregunta.

Consideremos cuatro formas de complejidad dinámica: dinámica caótica, límites de cuencas fractales, transiciones de fase discontinuas en situaciones de agentes heterogéneos y modelos teóricos de catástrofes relacionados con sistemas de agentes heterogéneos. Para el primero de ellos existe un problema claro para el agente, la existencia de una dependencia sensible de las condiciones iniciales. Si un agente pasa de la acción a it a la acción a jt , donde | a it  -  a jt | <  ε  <1, entonces no importa cuán pequeño sea ε , existe un m tal que | G ( f ( a it + t ′ ) - G ( f ( a jt + t ′ ) |> m para algún t ′ para cada ε . A medida que ε se acerca a cero, m / ε se acercará al infinito. Será muy difícil para el agente predecir el resultado de cambiar Este es el problema del efecto mariposa o la dependencia sensible de las condiciones iniciales. Más particularmente, si el agente tiene una conciencia imperfectamente precisa de sus acciones, con la zona de borrosidad superior a ε, el agente se enfrenta a un potencial amplio rango de incertidumbre con respecto al resultado de sus acciones. En Edward Lorenz (1963) estudio original de este asunto cuando "descubrió el caos", cuando reinició su simulación de un sistema de dinámica de fluidos de tres ecuaciones a la mitad, el error de redondeo que desencadenó una divergencia dramática posterior era demasiado pequeño para que su computadora lo "percibiera" ( en el cuarto decimal).

Hay dos elementos de compensación para la dinámica caótica. Aunque un conocimiento exacto es efectivamente imposible, requiriendo un conocimiento esencialmente infinitamente preciso (y conocimiento de ese conocimiento), un conocimiento aproximado más amplio a lo largo del tiempo puede ser posible. Por lo tanto, los sistemas caóticos generalmente están limitados y, a menudo, son ergódicos (aunque no siempre). Si bien las trayectorias relativas a corto plazo para dos acciones ligeramente diferentes pueden divergir bruscamente, las trayectorias volverán en algún momento posterior hacia la otra, acercándose arbitrariamente entre sí antes de volver a divergir. No sólo se pueden conocer los límites del sistema, sino que se puede conocer el promedio a largo plazo del sistema. Todavía hay límites, ya que uno nunca puede estar seguro de no estar lidiando con un largo transitorio del sistema, posiblemente moviéndose posteriormente a un modo de comportamiento sustancialmente diferente. Pero la posibilidad de un grado sustancial de conocimiento, incluso con cierto grado de confianza con respecto a ese conocimiento, no está fuera de discusión para los sistemas caóticamente dinámicos.

Con respecto a los límites de las cuencas fractales, identificados por primera vez para los modelos económicos por Hans-Walter Lorenz (1992) en el mismo artículo en el que discutió el problema de la fugacidad caótica. Mientras que en un sistema caótico puede haber solo una cuenca de atracción, aunque el atractor sea fractal y extraño y, por lo tanto, genere fluctuaciones erráticas, el caso del límite de la cuenca fractal involucra múltiples cuencas de atracción, cuyos límites entre sí toman formas fractales. El atractor para cada cuenca puede ser tan simple como ser un solo punto. Sin embargo, los límites entre las cuencas pueden encontrarse arbitrariamente cerca unos de otros en ciertas zonas.

En tal caso, para el caso puramente determinista, una vez que uno es capaz de determinar en qué cuenca de atracción se encuentra, puede resultar un grado sustancial de previsibilidad. Sin embargo, puede existir el problema de la dinámica transitoria, con el sistema tomando una ruta larga y tortuosa antes de comenzar a acercarse al atractor, incluso si el atractor es simplemente un punto al final. El problema surge si el sistema no es estrictamente determinista, si G incluye un elemento estocástico, por pequeño que sea. En este caso, uno puede ser empujado fácilmente a través del límite de una cuenca, especialmente si uno se encuentra en una zona donde los límites se encuentran muy cerca unos de otros. Por lo tanto, puede haber cambios discontinuos repentinos y muy difíciles de predecir en la trayectoria dinámica a medida que el sistema comienza a moverse hacia un atractor muy diferente en una cuenca diferente.

Sin embargo, también en este caso puede haber algo similar al tipo de dispensación a largo plazo que observamos para el caso de la dinámica caótica. Incluso si la predicción exacta en el caso caótico es casi imposible, es posible discernir patrones, límites y promedios más amplios. Asimismo, en el caso de los límites de cuencas fractales con un elemento estocástico, con el tiempo se debe observar un salto de una cuenca a otra. Algo parecido al patrón de dinámica de juegos evolutivos a largo plazo estudiado por Binmore y Samuelson (1999), uno puede imaginar a un observador haciendo un seguimiento de cuánto tiempo permanece el sistema en cada cuenca y eventualmente desarrollando un perfil de probabilidad del patrón, con el porcentaje de tiempo que el sistema pasa en cada cuenca posiblemente acercándose a los valores asintóticos. Sin embargo, esto depende de la naturaleza del proceso estocástico, así como del grado de complejidad del patrón fractal de los límites de la cuenca. Un proceso estocástico no ergódico puede hacer muy difícil, incluso imposible, observar la convergencia en un conjunto estable de probabilidades de estar en las respectivas cuencas, incluso si estas son pocas en número con atractores simples.

Para el caso de las transiciones de fase en sistemas de agentes heterogéneos que interactúan localmente, el mundo de la llamada "complejidad de carpa pequeña". Brock y Hommes (1997) han desarrollado un modelo útil para comprender tales transiciones de fase, basado en la mecánica estadística. Este es un sistema estocástico y está impulsado fundamentalmente por dos parámetros clave, una fuerza de interacciones o relaciones entre agentes vecinos y un grado de voluntad de cambiar patrones de comportamiento por parte de los agentes. Para su modelo, el producto de estos dos parámetros es crucial, con una bifurcación que ocurre para su producto. Si el producto está por debajo de un cierto valor crítico, habrá un solo estado de equilibrio. Sin embargo, una vez que este producto exceda un valor crítico particular, surgirán dos equilibrios distintos. Efectivamente, los agentes saltarán de un lado a otro entre estos equilibrios en los patrones de pastoreo. Para modelos de mercados financieros (Brock y Hommes1998) esto puede parecerse a oscilaciones entre mercados alcistas optimistas y mercados bajistas pesimistas, mientras que por debajo del valor crítico, el mercado tendrá mucha menos volatilidad, ya que rastrea algo que puede ser un equilibrio de expectativas racionales.

Para este tipo de configuración, existen esencialmente dos problemas graves. Uno es determinar el valor del umbral crítico. El otro es comprender cómo los agentes saltan de un equilibrio a otro en la zona de equilibrio múltiple. Ciertamente, el segundo problema se parece un poco a la discusión del caso anterior, si no involucra un conjunto tan dramático de posibles cambios discontinuos.

Por supuesto, una vez que se pasa un umbral de discontinuidad, puede ser reconocible cuando se vuelve a acercar. Pero antes de hacerlo, puede ser esencialmente imposible determinar su ubicación. El problema de determinar un umbral de discontinuidad es mucho más amplio y molesta a los legisladores en muchas situaciones, como intentar evitar umbrales catastróficos que pueden provocar el colapso de una población de especies o de todo un ecosistema. No se quiere cruzar el umbral, pero sin hacerlo no se sabe dónde está. Sin embargo, para situaciones menos peligrosas que involucran irreversibilidades, es posible determinar la ubicación del umbral a medida que uno se mueve hacia adelante y hacia atrás a través de él.

Por otro lado, en tales sistemas es muy probable que la ubicación de tales umbrales no permanezca fija. A menudo, tales sistemas exhiben un patrón evolutivo autoorganizado en el que los parámetros del sistema en sí mismos quedan sujetos a cambios evolutivos a medida que el sistema se mueve de una zona a otra. Tal falta de ergodicidad es consistente no solo con la incertidumbre del estilo keynesiano, sino que también puede llegar a parecerse a la complejidad identificada por Hayek (1948, 1967) en sus discusiones sobre la autoorganización dentro de sistemas complejos. Por supuesto, para las economías de mercado, Hayek mostró optimismo con respecto a los resultados de tales procesos. Incluso si los participantes del mercado no pueden predecir los resultados de tales procesos, el patrón de autoorganización será, en última instancia, muy beneficioso si se deja por sí solo. Aunque a menudo se considera que los keynesianos y los austriacos hayekianos están en profundo desacuerdo, algunos observadores han notado las similitudes de puntos de vista con respecto a estos fundamentos de la incertidumbre (Shackle1972; Loasby1976; Rosser Jr.2001a, B). Además, este enfoque conduce a la idea de la apertura de los sistemas que se vuelve consistente con el enfoque realista crítico de la epistemología económica (Lawson1997).

Considerar este problema de umbrales importantes nos lleva a la última de nuestras formas de complejidad dinámica para considerar aquí, las interpretaciones de la teoría de catástrofes. El problema del conocimiento es esencialmente el que se señaló anteriormente, pero se escribe más claramente en términos generales, ya que es más probable que las discontinuidades involucradas sean tan grandes como los estallidos de las grandes burbujas especulativas. El modelo de Brock-Hommes y sus descendientes pueden verse como una forma de lo que está involucrado, pero el enfoque de la teoría de catástrofes original saca a relucir cuestiones clave con mayor claridad.

La primera aplicación de la teoría de las catástrofes en economía por Zeeman (1974) de hecho consideraron las caídas de los mercados financieros en una formulación simplificada de dos agentes: fundamentalistas que estabilizaron el sistema comprando bajo y vendiendo caro y "chartistas" que persiguen tendencias de una manera desestabilizadora comprando cuando los mercados suben y vendiendo cuando caen. Al igual que en la formulación de Brock-Hommes, permite que los agentes cambien sus roles en respuesta a la dinámica del mercado, de modo que a medida que el mercado sube, los fundamentalistas se vuelven cartistas, acelerando la burbuja, y cuando llega el colapso, vuelven a ser fundamentalistas, acelerando el colapso. Rosser Jr. (1991) proporciona una formalización ampliada de esto en términos de teoría de catástrofes que lo vincula con el análisis de Minsky (1972) y Kindleberger (2001), retomado además en Rosser Jr. et al. (2012) y Rosser Jr. (2020c). Esta formulación implica una formulación catastrófica de cúspide en la que las dos variables de control son las demandas de las dos categorías de agentes, y la demanda de los cartistas determina la posición de la cúspide que permite las caídas del mercado.

El problema del conocimiento aquí involucra algo que no se modeló específicamente en Brock y Hommes, aunque tienen una versión del mismo. Se trata de las expectativas de los agentes sobre las expectativas de los demás agentes. Este es efectivamente el tema del "concurso de belleza" discutido por Keynes en el Capítulo 12 de esta Teoría General (1936). El ganador del concurso de belleza en un concurso de periódicos no es quien adivina a la chica más bonita, sino quien adivina mejor las suposiciones de los demás participantes. Keynes notó que uno podría comenzar a jugar a adivinar las expectativas de los demás en sus conjeturas de las conjeturas de los demás, y que esto podría ir a niveles más altos, en principio, una regresión infinita que conduce a un problema de conocimiento imposible. Por el contrario, el enfoque de Brock y Hommes simplemente tiene agentes que cambian de estrategia después de observar lo que hacen los demás. Estos problemas de nivel potencialmente superior no entran en juego. Este tipo de problemas reaparecen en los problemas asociados con la complejidad computacional.

## Conocimiento y ergodicidad {-}

Un tema controvertido que involucra el conocimiento y la complejidad involucra las fuentes profundas de la idea de Keynes-Knight de la incertidumbre fundamental (Keynes 1921; Caballero1921). Ambos dejaron en claro que para la incertidumbre no existe una distribución de probabilidad subyacente que determine eventos importantes sobre los que los agentes deben tomar decisiones. La formulación de Keynes de esto ha provocado mucha discusión y debate sobre por qué vio surgir esta falta de distribución de probabilidad.

Una teoría que ha recibido mucha atención, debido a Davidson (1982-83), es que si bien ni Keynes ni Knight lo mencionaron nunca, lo que puede provocar tal incertidumbre, especialmente para la comprensión de Keynes, es la aparición de noergodicidad en los procesos dinámicos subyacentes a la realidad económica. Al hacer este argumento, Davidson citó específicamente los argumentos de Paul Samuelson (1969, pag. 184) en el sentido de que "la economía como ciencia asume el axioma ergódico". Davidson se basó en esto para afirmar que el fracaso de este axioma es una cuestión ontológica que es fundamental para comprender la incertidumbre keynesiana, cuando el conocimiento se rompe. Muchos han repetido desde entonces este argumento, aunque Álvarez y Ehnts (2016) argumentan que Davidson malinterpretó a Samuelson, quien en realidad rechazó este punto de vista ergódico por estar vinculado a un punto de vista clásico más antiguo que él no aceptó.

El argumento de Davidson ha sido criticado más recientemente por varios observadores, quizás más enérgicamente recientemente por O'Donnell (2014-15), quien argumenta que Davidson ha tergiversado la hipótesis ergódica, que Keynes nunca la consideró y que la incertidumbre keynesiana es más una cuestión de inestabilidades a corto plazo que deben entenderse utilizando la economía del comportamiento en lugar de los elementos asintóticos vinculados a la ergodicidad. Un argumento importante de O'Donnell es que incluso en un sistema ergódico que va a pasar a un estado estacionario a largo plazo, puede estar fuera de ese estado durante un período de tiempo tan largo que uno no podrá determinar si es ergódico o no. Este es un fuerte argumento al que Davidson no ha logrado responder completamente (Davidson2015).

Para esto es fundamental comprender la hipótesis ergódica en sí misma y su desarrollo y límites, así como su relación con los propios argumentos de Keynes, que resulta algo complicado, pero de hecho vinculado a preocupaciones centrales de Keynes de manera indirecta, especialmente dado que nunca lo mencionó directamente. La mayoría de los economistas que discuten este asunto, incluidos Davidson y O'Donnell, han aceptado como definición de un sistema ergódico que a lo largo del tiempo (asintóticamente) sus "promedios espaciales son iguales a sus promedios temporales". Esta formulación se debió a Ehrenfest y Ehrenfest-Afanessjewa (1911), con Paul Ehrenfest alumno de Ludwig Boltzmann (1884) quien expandió el estudio de la ergodicidad (y acuñó el término) como parte de su largo estudio de la mecánica estadística, particularmente cómo un promedio agregado a largo plazo (como la temperatura) podría emerger de un conjunto de partes dinámicamente estocásticas (movimientos de partículas). Resulta que a pesar de su amplia influencia, la formulación precisa de los Ehrenfests era inexacta (Uffink2006). Pero esto reflejaba que había múltiples vertientes en el significado de "ergodicidad".

De hecho, existe un debate en curso sobre cómo Boltzmann acuñó el término en primer lugar. Su alumno, Ehrenfest, afirmó que era por combinar el griego ergos ("trabajo") con hodos ("camino"), mientras que Gallavotti (1999) que vino de él usando su propio neologismo, monode , que significa una distribución estacionaria, en lugar de hodos . Esto encaja con la mayoría de las primeras formulaciones de ergodicidad que lo analizaron dentro del contexto de distribuciones estacionarias.

Las discusiones posteriores sobre la ergodicidad se basarían en dos teoremas complementarios probados por Birkhoff (1931) y von Neumann (1932), aunque este último se probó primero y enfatiza la preservación de la medida, mientras que la variación de Birkhoff fue más geométrica y relacionada con las propiedades de recurrencia en sistemas dinámicos. Ambos implican convergencia a largo plazo, y la formulación de Birkhoff mostró no solo la preservación de la medida, sino que para un sistema ergódico estacionario una indecomponibilidad métrica tal que no solo se llena el espacio correctamente, sino que es imposible dividir el sistema en dos que también lo harán completamente. llenar el espacio y preservar la medida, resultado que extiende la obra fundamental de Poincaré (1890) sobre cómo la recurrencia y el llenado del espacio ayudan a explicar cómo pueden surgir dinámicas caóticas en la mecánica celeste.

En von Neumann's (1932) formulación sea T una transformación que preserva la medida en un espacio de medida con para cada función f integrable al cuadrado en ese espacio, ( Uf ) ( x ) =  f ( Tx ), entonces U es un operador unitario en el espacio. Para cualquier operador unitario U en un espacio de Hilbert H , la secuencia de promedios:

$$(1 / n)\left(f+U f+\cdots+U^{n-1} f\right)$$

está fuertemente convergente para cada f en H . Observamos que estos son espacios de medida finita y que esto se refiere a sistemas estacionarios, al igual que con Boltzmann.

Birkhoff's (1931), a veces llamado el "teorema ergódico individual", modifica la secuencia anterior de promedios para ser:

$$(1 / n)\left(f(x)+f(T x)+\cdots+f\left(T^{n-1 x}\right)\right)$$

que convergen para casi todas las x . Estos teoremas complementarios se han generalizado a los espacios de Banach y a muchas otras condiciones. 23 Fue a partir de estos teoremas que evolucionaría la próxima ola de desarrollos en Moscú y en otros lugares. 24 Este era el estado de la teoría ergódica cuando Keynes tuvo su debate sobre la econometría a fines de la década de 1930 con el alumno de Paul Ehrenfest, Jan Tinbergen.

El vínculo entre la estacionariedad y la ergodicidad llegaría a debilitarse en un estudio posterior, con Malinvaud (1966) mostrando que un sistema estacionario podría no ser ergódico, con un ciclo límite como ejemplo, con Davidson consciente de este caso desde el comienzo de sus discusiones. Sin embargo, se siguió creyendo que los sistemas ergódicos debían ser estacionarios, y esto siguió siendo una clave para Davidson, además de ser aceptado por la mayoría de sus críticos, incluido O'Donnell. Sin embargo, resulta que esto puede romperse en sistemas caóticos ergódicos de dimensión infinita, que pueden no ser estacionarios (Shinkai y Aizawa2006), que recupera el papel de la dinámica caótica en socavar la capacidad de lograr el conocimiento de un sistema dinámico, incluso uno que es ergódico.

Dadas estas complicaciones, vale la pena volver a Keynes para comprender cuáles eran sus preocupaciones, que salieron más claramente en sus debates con Tinbergen (1937, 1940; Keynes,1938) sobre cómo estimar econométricamente modelos para pronosticar la dinámica macroeconómica. Una profunda ironía aquí es que Tinbergen fue alumno de Paul Ehrenfest y, por lo tanto, fue influenciado por sus ideas sobre la ergodicidad, incluso cuando Keynes no abordó directamente este asunto. En cualquier caso, lo que Keynes objetó fue la aparente ausencia de homogeneidad, esencialmente una preocupación de que el modelo en sí cambia con el tiempo. La solución de Keynes a esto fue dividir una serie de tiempo en submuestras para ver si se obtienen las mismas estimaciones de parámetros que se obtienen para toda la serie de tiempo. La homogeneidad no es estrictamente idéntica ni a la estacionariedad ni a la ergodicidad, pero es probable que en ese momento Tinbergen, siguiendo a Ehrenfest, supusiera probablemente que las tres eran válidas para los modelos que estimó. Por lo tanto, se asumió que la hipótesis ergódica era válida para estos primeros modelos econométricos, mientras que Keynes se mostró escéptico de que existiera una homogeneidad suficiente para suponer que se sabía lo que estaba haciendo el sistema a lo largo del tiempo (Rosser Jr.2016a).

## Reflexividad y unificación de conceptos de complejidad {-}

Estrechamente relacionada con la autorreferencia está la idea de reflexividad . Este es un término sin una definición acordada, y se ha utilizado en una amplia variedad de formas (Lynch2000). Se deriva del latín reflectere , que generalmente se traduce como "inclinarse hacia atrás", pero puede referirse a "reflejo" como en una sacudida de la rodilla cuando se toca, no lo que se quiere decir aquí, o de manera más general está vinculado a "reflexión" como en una imagen que se refleja, posiblemente de un lado a otro muchas veces, como en la situación de dos espejos enfrentados. Este último es más el enfoque aquí y más el tipo que está conectado con la autorreferencia y todo lo que implica. Alguien que hizo ese vínculo con fuerza fue Douglas Hofstadter (1979) en su Gödel, Escher, Bach: An Eternal Golden Braid y aún más tarde (Hofstadter2006). Para Hofstadter, la reflexividad está ligada a los cimientos de la conciencia a través de lo que llama “bucles extraños” de autorreferencia indirecta, que para él destacan ciertos grabados de Maurits C. Escher, en particular sus “Dibujando manos” y también su “Galería de impresiones, ”Con muchos comentaristas sobre la reflexividad citando“ Drawing Hands ”, que muestra dos manos dibujándose entre sí (Rosser Jr.2020b). 25 Hofstadter sostiene que el fundamento de su teoría es el Teorema de incompletitud de Gödel, con su profunda autorreferencia, junto con ciertas piezas de JS Bach, así como estos grabados de Escher.

El término probablemente ha sido el más utilizado y con la mayor variedad de significados en sociología (Lynch 2000)). Su uso académico fue iniciado por el destacado sociólogo Robert K. Merton (1938), que lo utilizó para plantear el problema de los sociólogos pensando en cómo sus estudios y cavilaciones encajan en el marco social más amplio, tanto en cómo ellos mismos son influenciados por ese marco en términos de sesgos y paradigmas, pero también en términos de cómo sus estudios y la forma en que hacen sus estudios podría reflejarse también para influir en la sociedad. Entre los sociólogos, los usos más radicales del concepto implicaron una aguda autocrítica en la que uno deconstruye el paradigma e influye en el que está operando hasta el punto de que apenas se puede hacer ningún análisis (Woolgar1991), y muchos se quejan de que esto conduce a un callejón sin salida nihilista. Los primeros usos del término por los economistas siguieron esta línea particular de analizar cómo los economistas particulares están operando dentro de ciertos marcos metodológicos y cómo llegaron a hacerlo a partir de influencias sociales más amplias y cómo su trabajo puede luego reflejarse para influir en la sociedad, a veces incluso a través de determinadas influencias. políticas o incluso formas de recopilar y reportar datos relevantes para las políticas (Hands2001; Davis y Klaes2003).

Merton1948) también usaría la idea para proponer la idea de la profecía autocumplida , una idea que ha sido ampliamente aplicada en economía como con el concepto de equilibrio de manchas solares (Azariadis1981), y muchos ven esto como derivado originalmente de Keynes (1936, Cap. 12) y su análisis del comportamiento del mercado financiero basado en los concursos de belleza de los periódicos británicos de principios del siglo XX. En esos concursos, los periódicos publicaban fotos de mujeres jóvenes y pedían a los lectores que las calificaran por su presunta belleza. El ganador de tal concurso no fue la persona que adivinó qué mujer joven era objetivamente la más hermosa, sino cuál recibió la mayor cantidad de votos. Esto significaba que un jugador astuto de un juego de este tipo estaba realmente tratando de adivinar las conjeturas de los otros jugadores, y Keynes comparó esto con los mercados financieros donde el fundamental subyacente de un activo es menos importante para su valor de mercado de lo que los inversores creen que es. Esto llevó a Keynes incluso a notar que este tipo de razonamiento puede moverse a niveles más altos, tratando de pensar lo que otros piensan que piensan los demás. y a niveles aún más altos en una regresión infinita potencial, un reflejo infinito clásico en un programa que no se detiene. Esta idea de concurso de belleza de Keynes ha llegado a ser vista como una pieza central de su visión filosófica, lo que implica en última instancia no solo reflexividad sino también complejidad (Davis2017).

George Soros (1987), quien luego también argumentaría que el análisis era parte de la economía de la complejidad (Soros 2013). Soros ha argumentado durante mucho tiempo que pensar en esta versión de reflexividad inspirada en concursos de belleza ha sido clave para su propia toma de decisiones en los mercados financieros. Él lo ve como una explicación de los ciclos de auge y caída en los mercados como en la burbuja inmobiliaria estadounidense de principios de la década de 2000, cuyo declive desencadenó la Gran Recesión. Obtuvo el término por primera vez como estudiante de Karl Popper en la década de 1950 (Popper1959), con Popper también una influencia en Hayek (1967) en conexión con estas ideas (Caldwell 2013). Por lo tanto, la idea de reflexividad con vínculos a argumentos sobre la incompletitud y las regresiones infinitas asociadas con la autorreferencia se ha vuelto muy influyente entre los economistas y financieros que estudian la dinámica del mercado financiero y otros fenómenos relacionados.

Ahora vemos la posibilidad de vincular nuestras principales escuelas de complejidad a través del sutil y extraño bucle involucrado en la autorreferencia indirecta en el corazón de una forma más profunda de reflexividad. La autorreferencia indirecta en el corazón del teorema de incompletitud de Gödel está profundamente ligada a la complejidad computacional ya que conduce a los bucles do infinitos del nivel más alto de complejidad computacional en los que un programa nunca se detiene. La salida de la incompletitud implica, en efecto, lo que Davis y Klaes invocaron: pasar a un nivel jerárquico superior en el que un agente o programa exógeno determina qué es verdadero o falso, aunque esto abre la puerta a la incoherencia (Landini et al.2020). La autorreferencia indirecta abre la puerta a la complejidad dinámica en sus implicaciones para la dinámica del mercado, lo que también se vincula a la complejidad jerárquica a medida que se pueden generar nuevos niveles de jerarquía. Consideremos brevemente cómo surge esto del fundamental Gödel (1931) teorema.

El teorema de Gödel es en realidad dos teoremas. El primero es el de incompletitud: cualquier sistema formal consistente en el que se pueda realizar aritmética elemental 26 es incompleto; hay enunciados en el lenguaje del sistema formal que no pueden probarse ni refutarse dentro del sistema formal. El segundo aborda el problema de la coherencia 27: para cualquier sistema formal consistente en el que se pueda realizar aritmética elemental, la consistencia del sistema formal no puede probarse dentro del sistema formal mismo. Entonces, la coherencia implica lo incompleto, pero cualquier intento de superar lo incompleto moviéndose a un nivel superior implica que uno no pueda probar la consistencia de este sistema de nivel superior, y ambas partes de esta falla se deben a las paradojas de la autorreferencia (reflexiva) que conduce a paradojas.

Hofstadter (2006) proporciona una excelente discusión de la naturaleza del carácter indirecto involucrado en la demostración de la parte principal del teorema, que implica el uso de "números de Gödel". Estos son números asignados a enunciados lógicos, y su uso puede conducir a la creación de enunciados paradójicos autorreferenciados incluso dentro de un sistema especialmente diseñado para evitar tales enunciados autorreferenciales. El sistema al que Gödel sometió este tratamiento eventualmente genera un enunciado equivalente a “Esta oración es indemostrable” fue el sistema lógico desarrollado por Whitehead y Russell (1910-13) específicamente para proporcionar una base formal consistente para las matemáticas sin paradojas lógicas. Russell, en particular, estaba muy preocupado por la posibilidad de paradojas en la teoría de conjuntos, como las que involucran conjuntos de autorreferencia. El problema clásico era "¿El conjunto de todos los conjuntos que no se contienen a sí mismos se contiene a sí mismo?" Una famosa versión simple de esto implica "¿Quién afeita al barbero en una ciudad donde el barbero solo afeita a los que no se afeitan ellos mismos?" Ambos implican bucles de trabajo interminables similares que surgen de su autorreferencia. Whitehead y Russell intentaron eliminar estas molestias desarrollando la teoría de tipos que establecía jerarquías de conjuntos para evitar que se refirieran a sí mismos. Pero luego Gödel hizo su truco de establecer sus números, que aplicó al sistema de Whitehead y Russell de manera indirecta para generar una declaración autorreferencial que implicaba una paradoja irresoluble dentro del sistema. Es más bien como el agujero que hizo Escher en medio de su “Galería de Grabados” permitió que el hombre mirara un grabado en una pared en una galería de una ciudad que contiene la galería en la que está parado mirándola.

Por lo tanto, no es sorprendente que el problema de la autorreferencia haya sido el núcleo de gran parte del pensamiento sobre la reflexividad desde un punto temprano, y que este pensamiento adquirió un tono más agudo cuando varias figuras pensaron en el teorema de Gödel, o incluso antes, en el teorema de Gödel. las paradojas consideradas por Bertrand Russell. Vincular esto a la comprensión de la complejidad proporciona una base para una complejidad reflexiva que abarca todas las formas principales de complejidad.

## Observaciones adicionales {-}

En los sistemas computacionalmente complejos el problema de comprenderlos está relacionado con la lógica, los problemas de regresión infinita e indecidibilidad asociados con la autorreferencia en sistemas de máquinas de Turing. Esto puede manifestarse como el problema de la detención, algo que puede surgir incluso para una computadora que intenta calcular con precisión incluso un sistema dinámicamente complejo como, por ejemplo, la forma exacta del conjunto de Mandelbrot (Blum et al.1998). Una máquina de Turing no puede comprender completamente un sistema en el que su propia toma de decisiones es una parte crucial. Sin embargo, el conocimiento de tales sistemas puede obtenerse por otros medios.

En la medida en que los modelos tienen fundamentos axiomáticos en lugar de ser meramente ad hoc, como muchos de ellos en última instancia lo son, estos fundamentos están estrictamente dentro del modo matemático clásico no constructivista, asumiendo el axioma de elección, la ley del medio excluido y otros caballos de batalla de los matemáticos y economistas matemáticos cotidianos. En la medida en que brinden información sobre la naturaleza de la complejidad económica dinámica y el problema especial de la emergencia (o anagénesis), no lo hacen basándose en fundamentos axiomáticos 28eso pasaría bien con los constructivistas e intuicionistas de principios y mediados del siglo XX, mucho menos con sus discípulos más recientes, que siguen la esperanza ideal de que “El futuro es una minoría; el pasado y el presente son mayoría ”, para citar a Velupillai (2005b, pag. 13), él mismo parafraseando a Shimon Peres de una entrevista sobre las perspectivas de paz en Oriente Medio.

Existe una gama considerable de modelos disponibles para contemplar o modelar fenómenos emergentes que operan en diferentes niveles jerárquicos. Un área interesante para ver cuál de los enfoques podría resultar más adecuado podría ser el estudio de la evolución de los procesos del mercado a medida que ellos mismos se vuelven más informatizados. Este es el enfoque de Mirowski (2007) que llega a afirmar que fundamentalmente los mercados son algoritmos. El tipo simple de precio publicado: mercado al contado en el que la mayoría de la gente tradicionalmente ha comprado cosas se encuentra en la parte inferior de una jerarquía chomskyiana de complejidad y control autoreferenciado. Así como los algoritmos más nuevos pueden contener algoritmos más antiguos dentro de ellos, la aparición de tipos más nuevos de mercados puede contener y controlar los tipos más antiguos a medida que avanzan hacia niveles más altos en esta jerarquía chomskyiana. Los mercados de futuros pueden controlar los mercados al contado, los mercados de opciones pueden controlar los mercados de futuros, y el orden cada vez más alto de estos mercados y su creciente automatización empuja al sistema a un nivel más alto hacia el ideal inalcanzable de ser una Máquina Universal de Turing en toda regla (Cotogno2003).

Mirowski aporta argumentos más recientes en biología con respecto a la coevolución, señalando que el espacio en el que los agentes y sistemas están evolucionando cambia con su evolución. En la medida en que el sistema de mercado se asemeje cada vez más a un conjunto gigantesco de algoritmos que interactúan y evolucionan, tanto la biología como el problema de la computabilidad se manifestarán e influirán mutuamente (Stadler et al.2001). Al final, la distinción entre los dos puede volverse irrelevante.

En el gran contraste de la complejidad computacional y dinámica, vemos superposiciones cruciales que involucran cómo las paradojas que surgen de la autorreferenciación de la complejidad computacional subyacente pueden implicar el surgimiento tan profundamente asociado con la complejidad dinámica. Estas interrelaciones pueden volverse más manifiestas al contemplar el mundo espejo de la reflexividad y sus interminables concatenaciones. Estas son algunas de las muchas consideraciones que se encuentran en los cimientos de la economía de la complejidad.

## Notas {-}

1. Velupillai2011) ha etiquetado esta visión de la complejidad dinámica como complejidad “Day-Rosser”.

2. Estrictamente hablando, esto es incorrecto. Goodwin1947) mostraron tales patrones dinámicos endógenos en sistemas lineales acoplados con rezagos. Sistemas similares fueron analizados por Turing (1952) en su trabajo que ha sido visto como el fundamento de la teoría de la morfogénesis, un fenómeno de complejidad por excelencia. Sin embargo, la inmensa mayoría de estos sistemas dinámicamente complejos implican cierta no linealidad, y el equivalente normalizado desacoplado del sistema lineal acoplado no es lineal.

3. Esta moneda vino de Horgan (1997, Cap. 11) quien etiquetó con desdén las cuatro C para representar la caoplexidad , que él consideraba una burbuja intelectual o una moda pasajera. Rosser Jr. (1999) argumentó que se trataba de una acuñación como “Impresionismo” que inicialmente fue un insulto pero que puede verse como una caracterización útil.

4. Arnol'd (1993) proporciona una discusión clara de los problemas matemáticos involucrados mientras evita las controversias.

5. Para una discusión más detallada de las controversias matemáticas subyacentes que involucran la teoría del caos, ver Rosser Jr. (2000b, Apéndice matemático).

6 .
Este término fue acuñado por Abraham y Shaw (1987) y Abraham (1985) también concibió el fenómeno combinado relacionado de caostrofe .

7 .
A menudo se ha afirmado incorrectamente que Schelling utilizó un tablero de ajedrez para este estudio.

8 .
La complejidad estructural parece al final equivaler a una "complejidad", que Israel (2005) argumenta es meramente un concepto epistemológico más que ontológico, con "complejidad" y "complicación" provenientes de diferentes raíces latinas ( complecti , "captar, comprender o abrazar" y complicare , "doblar, envolver"), incluso si muchos confundiría los conceptos (incluso von Neumann1966). Rosser Jr. (2004) argumenta que la complejidad como tal plantea problemas epistemológicos esencialmente triviales, cómo descifrar muchas partes diferentes y sus vínculos.

9 .
La “economía computable” fue neologizada por Velupillai en 1990 y se distingue de la “economía computacional”, simbolizada por el trabajo que se encuentra en las conferencias de la Association for Computational Economics y su revista Computational Economics. El primero se centra más en los fundamentos lógicos del uso de las computadoras en la economía, mientras que el segundo tiende a centrarse más en aplicaciones y métodos específicos.

10 .
Otro tema principal de la economía computable implica considerar qué partes de la teoría económica pueden probarse cuando se relajan axiomas lógicos clásicos como el axioma de elección y la exclusión del medio. Bajo tales matemáticas constructivas pueden surgir problemas para probar los equilibrios walrasianos (Pour-El y Richards1979; Richter y Wong1999; Velupillai2002, 2006) y equilibrios de Nash (Prasad 2005).

11 .
Debe entenderse que mientras que, por un lado, el primer trabajo de Kolmogorov axiomatizaba la teoría de la probabilidad, sus esfuerzos por comprender el problema de la inducción lo llevarían a argumentar más tarde que la teoría de la información precede a la teoría de la probabilidad (Kolmogorov 1983). McCall (2005) proporciona una discusión útil de esta evolución de las opiniones de Kolmogorov.

12 .
A Albin le gustó el ejemplo del problema de agregación de capital planteado por Joan Robinson (1953-54) que para agregar capital es necesario conocer ya el producto marginal del capital para determinar la tasa de descuento para calcular los valores presentes, mientras que al mismo tiempo ya se necesita conocer el valor del capital agregado para determinar su valor marginal. producto. La economía convencional intenta escapar de este bucle do potencialmente infinito simplemente asumiendo que todos estos se resuelven convenientemente simultáneamente en un gran equilibrio general.

13 .
Estrechamente relacionado estaría el prior universal de Solomonoff (1964) que coloca el concepto MDL en un marco bayesiano. De aquí surge la idea bastante intuitiva de que el estado más probable también tendrá la longitud más corta de algoritmo para describirlo. El trabajo de Solomonoff también se desarrolló de forma independiente, basándose en la teoría de la probabilidad de Keynes (1921).

14 .
El problema P = NP fue identificado por primera vez por John Nash Jr. (1955) en una carta a la Agencia de Seguridad Nacional de EE. UU. que analiza los métodos de cifrado en el criptoanálisis, que se clasificó hasta 2013. Nash dijo que pensaba que era cierto que P no era igual a NP, pero señaló que no pudo probarlo, y aún no se ha comprobado que este día.

15 .
Irónicamente, la prueba original de Brouwer de su teorema del punto fijo se basó en los axiomas de ZFC, y él solo proporcionó una alternativa intuicionista mucho más tarde (Brouwer 1952).

16 .
Para discusiones lógicas autorizadas de los temas involucrados en general en estas alternativas constructivistas, ver Kleene y Vesley. 1965; Kleene1967; obispo1967).

17 .
Este término se ha asociado especialmente con Bak (1996) y su criticidad autoorganizada , aunque no fue el primero en discutir la autoorganización en estos contextos.

18 .
El argumento de McCauley se basa en Moore (1990, 1991a,B) estudio de mapas iterados de baja dimensión que son máquinas de Turing sin atractores, propiedades de escala o dinámica simbólica. McCauley sostiene que este punto de vista proporciona una base para la complejidad como sorpresa final e imprevisibilidad.

19 .
En un modelo relacionado, Holden y Erneux (1993) muestran que el cambio sistémico puede tomar la forma de un paso lento a través de una bifurcación de Hopf supercrítica, lo que conduce a la persistencia durante un tiempo del estado anterior incluso después de que se ha pasado el punto de bifurcación.

20 .
Otro enfoque más involucra la idea del hiperciclo debido a Eigen y Schuster (1979), discutido en el próximo capítulo.

21 .
Ver Vaughn (1999), Vriend (2002) y Caldwell (2004) para discutir cómo llegó Hayek a sus puntos de vista sobre la complejidad y la emergencia y cómo encajan con sus otros puntos de vista.

22 .
La oposición a la planificación centralizada y el apoyo al surgimiento espontáneo de sistemas de mercado de abajo hacia arriba se muestra en un largo debate entre filósofos sobre si el surgimiento solo funciona de abajo hacia arriba o si puede involucrar una causalidad de arriba hacia abajo. Van Cleve (1990) introduce la supervención al permitir esta causalidad de arriba hacia abajo en los sistemas emergentes, mientras que Kim (1999) sostiene que los procesos emergentes deben ser fundamentalmente de abajo hacia arriba. Luis (2012) argumenta que Hayek se movió hacia el punto de vista de la supervención en sus escritos posteriores que también enfatizaban los procesos evolutivos grupales (Rosser Jr. 2014b).

23 .
Ver Halmos (1958) sobre cómo estos teoremas vinculan la teoría de la medida con la teoría de la probabilidad.

24 .
Velupillai2013, págs. 432-433, n8) muestra que, si bien la mayoría de la teoría ergódica ha seguido una formulación frecuentista, la Escuela de Moscú se basaría en las ideas de Keynes en su enfoque de estos temas.

25 .
A menudo se piensa que los ejemplos de reflexividad en el arte implican el efecto Droste , en el que una obra contiene una imagen de sí misma dentro de sí misma, claramente una cuestión de autorreferencia. Entre los primeros ejemplos conocidos se encuentra un cuadro de Giotto de 1320, El tríptico de Stefaneschi , en el que en el panel central se representa al cardenal Stefaneschi arrodillado ante San Pedro y presentándole el tríptico mismo. No hace falta decir que, incluso si dejan de representarse después de una secuencia finita de imágenes, tales obras de arte que exhiben este efecto Droste implican una regresión infinita de imágenes cada vez más pequeñas que contienen imágenes cada vez más pequeñas (Rosser Jr.2020b).

26 .
Por "aritmética elemental" se entiende aquello que puede derivarse del conjunto de axiomas de Peano asumiendo una lógica estándar del tipo Zermelo-Frankel con el axioma de elección (ZFC).

27 .
Cabe señalar que en su teorema original, Gödel solo pudo demostrar que estaba incompleto para una forma limitada de consistencia ω. Rosser Sr. (1936) que usó la "Oración de Rosser" (o "truco"): "Si esta oración es demostrable, entonces hay una prueba más corta de su negación". Esto ha llevado a algunos a referirse al teorema combinado como el "Teorema de Gödel-Rosser".

28 .
Si bien este movimiento se centra en refinar los fundamentos axiomáticos, en última instancia busca ser menos formalista y bourbakiano. Esto es consistente con la historia de la economía matemática, que primero se movió hacia una mayor axiomatización y formalismo dentro del paradigma matemático clásico, solo para alejarse de él en años más recientes (Weintraub2002).